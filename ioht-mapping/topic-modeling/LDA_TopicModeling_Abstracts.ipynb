{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_TopicModeling_Abstracts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGMhi0BUOBRT"
      },
      "source": [
        "## **LDA documentation**\n",
        "\n",
        "> https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
        "\n",
        "## **Tutorials**\n",
        "\n",
        "> https://shravan-kuchkula.github.io/topic-modeling/#diagnose-model-performance-using-perplexity-and-log-likelihood\n",
        "> https://github.com/NeverForged/LDATopicCoherence/blob/master/TopicCoherence.ipynb\n",
        "> https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
        "> https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wFyE4LFr9bs"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jLCaPL1sP7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "63b532ea-6dd9-4a9d-e05b-4f7df06c52a0"
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install pyldavis\n",
        "\n",
        "import csv\n",
        "%matplotlib inline \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "# from irlb import irlb\n",
        "from scipy import stats\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.patches as mpatches\n",
        "import scipy.spatial.distance as scdist\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "#from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "sb.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "import statsmodels.sandbox.stats.multicomp as mc\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "from nltk.stem import RSLPStemmer\n",
        "from textblob import TextBlob\n",
        "import collections\n",
        "import unidecode\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "# Load the LDA model from sk-learn\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "# from pyLDAvis import sklearn as sklearn_lda\n",
        "import pickle \n",
        "import os\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "# Load the library with the CountVectorizer method\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import joblib\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting pyldavis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyldavis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyldavis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyldavis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyldavis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyldavis) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (8.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (49.2.0)\n",
            "Building wheels for collected packages: pyldavis, funcy\n",
            "  Building wheel for pyldavis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldavis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=4c71a7365b0ed86a2d9871637f34c76066d3c864dc1beb101d27b6f49723589c\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=9ef53af65a0a94199a73ff8062e292af60c326a27b64af9acd6029d87dd20ed5\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyldavis funcy\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-1.14 pyldavis-2.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY60kJX-FPYG"
      },
      "source": [
        "## **Topic Coherence Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr4mh16lFWen"
      },
      "source": [
        "# https://github.com/NeverForged/LDATopicCoherence/blob/master/TopicCoherence.ipynb\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class TopicCoherence(object):\n",
        "    '''\n",
        "    Based on the information here: http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
        "    Calculating the topic coherence for LDA through sklearn, rather than through Gensim\n",
        "    \n",
        "    ATTRIBUTES\n",
        "    D: Gives the Document counts, with D(wi) on the diagonal and D(wi,wj) as i,j\n",
        "    p: Gives the probabilities, with p(wi) on the diagonal and the rest as p(wi,wj)\n",
        "    vocabulary: saved the vocabulary from the fit, so we know what word is what\n",
        "    UCI_score: UCI Score (wi,wj)\n",
        "    UMass_score: UMass Score(wi,wj)\n",
        "    \n",
        "    METHODS:\n",
        "    fit(vocabulary, documents): Creates the Document Counts and the probability\n",
        "        counts for the corpus and vectorizer used.\n",
        "        vocabulary: CountVectorizer.vocabulary_\n",
        "        documents: the raw documents used, based on what was put into\n",
        "                    the count vectorizer.\n",
        "     \n",
        "    \n",
        "    Darin LaSota, 2/7/2019\n",
        "    \n",
        "    '''\n",
        "    def __init__(self,words=10,score='both'):\n",
        "        '''\n",
        "        Initializer.\n",
        "        '''\n",
        "        self.words_to_use = words\n",
        "        self.score = score\n",
        "\n",
        "    def fit(self, vocabulary, transformed_docs, verbose=False):\n",
        "        '''\n",
        "        This is to get the various document probabilities, likely because these will be the same \n",
        "        for all models if running this in a grid_search\n",
        "        '''\n",
        "        self.verbose = verbose\n",
        "        if verbose:\n",
        "            print('Starting...')\n",
        "        self.vocabulary = vocabulary  # save for later\n",
        "        self.docs_words = transformed_docs > 0\n",
        "        self.docs_words = self.docs_words*1.0\n",
        "        self.Di = np.sum(self.docs_words,0)\n",
        "        if verbose:\n",
        "            print('Di done')\n",
        "        self.Dij = self.docs_words.transpose() * self.docs_words\n",
        "        if verbose:\n",
        "            print('Dij done')       \n",
        "        self.pi = self.Di/transformed_docs.shape[0]\n",
        "        self.pij = self.Dij/transformed_docs.shape[0]\n",
        "        if verbose:\n",
        "            print('pi and pij done')\n",
        "        # save the scores as made, to avoid redunency\n",
        "        self.UCI_score = csr_matrix(self.Dij.shape)\n",
        "        self.UMass_score = csr_matrix(self.Dij.shape)\n",
        "    \n",
        "    def UCI(self,i,j):\n",
        "        '''\n",
        "        Calculates the following:\n",
        "                score(wi,wj) = log(1 + p(wi,wj)/p(wi)p(wj))\n",
        "        Added the smoothing factor of 1 to keep results positive (and not heading toward \n",
        "        negative infinity)\n",
        "        '''\n",
        "        if i>j:\n",
        "            a = i\n",
        "            i = j\n",
        "            j = a\n",
        "        if self.UCI_score[i,j] == 0:\n",
        "            self.UCI_score[i,j] = np.log(1 + self.pij[i,j]/(self.pi[0,i]*self.pi[0,j]))\n",
        "        return self.UCI_score[i,j]\n",
        "                      \n",
        "    def UMass(self,i,j):\n",
        "        '''\n",
        "        Calculates the following:\n",
        "            score(wi,wj) = log(1 + D[wi,wj]/D[wi])\n",
        "        \n",
        "        '''\n",
        "        if self.UMass_score[i,j] == 0:\n",
        "            self.UMass_score[i,j] = np.log(1+self.Dij[i,j]/self.Di[0,i])\n",
        "        return self.UMass_score[i,j]\n",
        "\n",
        "    def myTCScore(self,model):\n",
        "        score = []\n",
        "        for topic_weights in model.components_:\n",
        "            top_keyword_locs = (-topic_weights).argsort()[:self.words_to_use]\n",
        "            for word in top_keyword_locs:\n",
        "                if self.score == 'UCI' or self.score == 'both':\n",
        "                    uci = np.mean([self.UCI(word,a) for a in  top_keyword_locs if a != word])\n",
        "                if self.score == 'UMass' or self.score == 'both':\n",
        "                    umass = np.mean([self.UMass(word,a) for a in  top_keyword_locs if a != word])\n",
        "                if self.score == 'UMass':\n",
        "                    score.append(umass)\n",
        "                elif self.score == 'UCI':\n",
        "                    score.append(uci)\n",
        "                else:\n",
        "                    score.append(uci*umass)\n",
        "        if self.verbose:\n",
        "            print('Mean {} Score: {:.2f}'.format(self.score, np.mean(score)), end='\\r')\n",
        "        return np.mean(score)\n",
        "        \n",
        "    def tc_score(self,model,X,y=[]):\n",
        "        score = []\n",
        "        for topic_weights in model.components_:\n",
        "            top_keyword_locs = (-topic_weights).argsort()[:self.words_to_use]\n",
        "            for word in top_keyword_locs:\n",
        "                if self.score == 'UCI' or self.score == 'both':\n",
        "                    uci = np.mean([self.UCI(word,a) for a in  top_keyword_locs if a != word])\n",
        "                if self.score == 'UMass' or self.score == 'both':\n",
        "                    umass = np.mean([self.UMass(word,a) for a in  top_keyword_locs if a != word])\n",
        "                if self.score == 'UMass':\n",
        "                    score.append(umass)\n",
        "                elif self.score == 'UCI':\n",
        "                    score.append(uci)\n",
        "                else:\n",
        "                    score.append(uci*umass)\n",
        "        if self.verbose:\n",
        "            print('Mean {} Score: {:.2f}'.format(self.score, np.mean(score)), end='\\r')\n",
        "        return np.mean(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9OsS9N9tHUT"
      },
      "source": [
        "## **Getting dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GttYItt9tKCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cf849fd1-ef8e-4bca-a942-5bd78bc6a8f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJD9iXZtUrZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a89ef384-5237-4305-d43a-1090ffef147c"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 500)\n",
        "df = pd.read_csv('/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/dataset/20200806_AllAcceptedPapers.csv', sep=',', quoting=csv.QUOTE_ALL)\n",
        "df = df.drop(columns=['Author', 'Venue', 'Venue Type', 'Impact Factor', 'Journal', 'DOI', 'Year', 'DocType', 'Source', 'Evaluation', 'Our clustering', 'First Author Name', 'First Author Country', 'Publication Source', 'Research Type', 'Empirical Validation', 'Type of Solution', 'Contribution Type', 'User Profile'], axis=1)\n",
        "df['text'] = df[\"Title\"] + ' ' + df[\"Abstract\"]\n",
        "df = df.drop(columns=['Title', 'Abstract'], axis=1)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD1akFRWrswv"
      },
      "source": [
        "## **Preprocessing tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1-bc4dr1KJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a0f7b51e-b2e9-4605-e238-5a12dfbb0901"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#stops = set(stopwords.words(\"english\"))\n",
        "file_stops = open('/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/stop_words.txt')\n",
        "file_stops = file_stops.read()\n",
        "stops = file_stops.split()\n",
        "print(\"Stopwords:\", len(stops))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Stopwords: 1123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcfHsXOt4Kf9"
      },
      "source": [
        "# palavras pequenas e forçar singular\n",
        "min_length = 3\n",
        "def textblob_tokenizer(str_input):\n",
        "    global N, execucoes\n",
        "    text = str_input\n",
        "    letters_only = re.sub(u'[^a-zA-ZáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', ' ', text)\n",
        "    tokens = nltk.word_tokenize(letters_only.lower())\n",
        "    tokens = [unidecode.unidecode(token) for token in tokens if not token in stops and len(token) >= min_length]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]    \n",
        "    text = ' '.join(tokens)\n",
        "    if len(text) > 3:\n",
        "      return text\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVknkC1l4kTc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "580f0695-e942-4c76-d6ba-49334851a6ec"
      },
      "source": [
        "abstracts = df['text'].str.lower()\n",
        "df['text_processed'] = [textblob_tokenizer(abstract) for abstract in abstracts]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>194213</td>\n",
              "      <td>Cross-Sectoral Big Data: The Application of an Ethics Framework for Big Data in Health and Research Discussion of uses of biomedical data often proceeds on the assumption that the data are generated and shared solely or largely within the health sector. However, this assumption must be challenged because increasingly large amounts of health and well-being data are being gathered and deployed in cross-sectoral contexts such as social media and through the internet of (medical) things and wear...</td>\n",
              "      <td>cross sectoral big data application ethic framework big data health research discussion biomedical data proceeds assumption data generated shared solely largely health sector assumption challenged increasingly large amount health data gathered deployed cross sectoral context social medium internet medical thing wearable device cross sectoral sharing data refers generation linkage biomedical data health sector paper considers challenge arise phenomenon benefit fully important ethical value st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>194209</td>\n",
              "      <td>Internet of things (IoT) applications for elderly care: a reflective review Increasing in elderly population put extra pressure on healthcare systems globally in terms of operational costs and resources. To minimize this pressure and provide efficient healthcare services, the application of the Internet of Things (IoT) and wearable technology could be promising. These technologies have the potential to improve the quality of life of the elderly population while reducing strain on healthcare ...</td>\n",
              "      <td>internet thing iot application elderly care reflective review increasing elderly population put extra pressure healthcare system globally term operational cost resource minimize pressure efficient healthcare service application internet thing iot wearable technology promising technology potential improve quality life elderly population reducing strain healthcare system minimizing operational cost iot wearable application elderly healthcare purpose reviewed previously summarize current applic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>194182</td>\n",
              "      <td>Wearable hardware design for the internet of medical things (IoMT) As the life expectancy of individuals increases with recent advancements in medicine and quality of living, it is important to monitor the health of patients and healthy individuals on a daily basis. This is not possible with the current health care system in North America, and thus there is a need for wireless devices that can be used from home. These devices are called biomedical wearables, and they have become popular in t...</td>\n",
              "      <td>wearable hardware design internet medical thing iomt life expectancy individual increase recent advancement medicine quality living important monitor health patient healthy individual daily basis current health care system north america wireless device home device biomedical wearable popular decade reason main expensive health care longer wait time increase public awareness improving quality life vital wearable understanding designed significance factor considered hardware designed study att...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>194165</td>\n",
              "      <td>Biosignal monitoring using wearables: Observations and opportunities Advances in data acquisition technologies, sensor design, data frameworks, smart device connectivities, Internet-of-things, rising health care costs and public awareness towards a better quality of life, have spurred a boom in development of wearable \"health-tech\" devices in the smart device market. Tele-monitoring of human body dynamics through activities of daily life has become a popular lifestyle choice for consumers, a...</td>\n",
              "      <td>biosignal monitoring wearable observation opportunity advance data acquisition technology sensor design data framework smart device connectivity internet thing rising health care cost public awareness quality life spurred boom development wearable health tech device smart device market tele monitoring human body dynamic activity daily life popular lifestyle choice consumer help track parameter food intake calorie burnt activity level calling nearest health care facility emergency device give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>194110</td>\n",
              "      <td>Liquid level sensing using commodity wifi in a smart home environment The popularity of Internet-of-Things (IoT) has provided us with unprecedented opportunities to enable a variety of emerging services in a smart home environment. Among those services, sensing the liquid level in a container is critical to building many smart home and mobile healthcare applications that improve the quality of life. This paper presents LiquidSense, a liquid level sensing system that is low-cost, high accurac...</td>\n",
              "      <td>liquid level sensing commodity wifi smart home environment popularity internet thing iot provided unprecedented opportunity enable variety emerging service smart home environment service sensing liquid level container critical building smart home mobile healthcare application improve quality life paper present liquidsense liquid level sensing system low cost high accuracy widely applicable daily liquid container easily integrated existing smart home network liquidsense existing home wifi net...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text_processed\n",
              "0  194213  ...  cross sectoral big data application ethic framework big data health research discussion biomedical data proceeds assumption data generated shared solely largely health sector assumption challenged increasingly large amount health data gathered deployed cross sectoral context social medium internet medical thing wearable device cross sectoral sharing data refers generation linkage biomedical data health sector paper considers challenge arise phenomenon benefit fully important ethical value st...\n",
              "1  194209  ...  internet thing iot application elderly care reflective review increasing elderly population put extra pressure healthcare system globally term operational cost resource minimize pressure efficient healthcare service application internet thing iot wearable technology promising technology potential improve quality life elderly population reducing strain healthcare system minimizing operational cost iot wearable application elderly healthcare purpose reviewed previously summarize current applic...\n",
              "2  194182  ...  wearable hardware design internet medical thing iomt life expectancy individual increase recent advancement medicine quality living important monitor health patient healthy individual daily basis current health care system north america wireless device home device biomedical wearable popular decade reason main expensive health care longer wait time increase public awareness improving quality life vital wearable understanding designed significance factor considered hardware designed study att...\n",
              "3  194165  ...  biosignal monitoring wearable observation opportunity advance data acquisition technology sensor design data framework smart device connectivity internet thing rising health care cost public awareness quality life spurred boom development wearable health tech device smart device market tele monitoring human body dynamic activity daily life popular lifestyle choice consumer help track parameter food intake calorie burnt activity level calling nearest health care facility emergency device give...\n",
              "4  194110  ...  liquid level sensing commodity wifi smart home environment popularity internet thing iot provided unprecedented opportunity enable variety emerging service smart home environment service sensing liquid level container critical building smart home mobile healthcare application improve quality life paper present liquidsense liquid level sensing system low cost high accuracy widely applicable daily liquid container easily integrated existing smart home network liquidsense existing home wifi net...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltdvb6bL6WLg"
      },
      "source": [
        "df.to_csv(r'/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/dataset/text-processed.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyMzZRtPKr8A"
      },
      "source": [
        "## **Grid Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxBvzPGgKz14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "75362fd8-9d5e-4aa8-ad5e-6eecf05fb376"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define Search Param\n",
        "search_params = {\n",
        "    'n_components':[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n",
        "    'learning_decay':[0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "    'learning_offset':[8, 9, 10, 11, 12],\n",
        "    'max_iter':[5, 10, 15, 20],\n",
        "}\n",
        "# Init the model\n",
        "ldaS = LDA()\n",
        "# Init Grid Search class\n",
        "model = GridSearchCV(ldaS, search_params)\n",
        "# Do the Grid Search\n",
        "model.fit(count_data)\n",
        "\n",
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "# Perplexity\n",
        "print(\"Best Model Perplexity: \", best_lda_model.perplexity(count_data))\n",
        "# Topic Coherence\n",
        "print(\"Best Topic Coherence: \", Topic_Coherence.myTCScore(best_lda_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Model's Params:  {'learning_decay': 0.5, 'learning_offset': 11, 'max_iter': 15, 'n_components': 4}\n",
            "Best Log Likelihood Score:  -12658.431383193298\n",
            "Best Model Perplexity:  305.1354911864333\n",
            "Mean both Score: 0.32\rBest Topic Coherence:  0.32160021460508714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECBPtaNr3qP"
      },
      "source": [
        "## **LDA Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvqqDMpmr7sc"
      },
      "source": [
        "# Helper function\n",
        "def print_topics(model, count_vectorizer, n_top_words):\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        \n",
        "def generate_topics(count_vectorizer, count_data, number_topics, number_words):\n",
        "  # Create and fit the LDA model\n",
        "  lda = LDA(n_components=number_topics, learning_decay=0.5, learning_offset=11, max_iter=15, n_jobs=-1)\n",
        "  lda.fit(count_data)\n",
        "\n",
        "  # Print the topics found by the LDA model\n",
        "  #print(\"Topics found via LDA:\")\n",
        "  #print_topics(lda, count_vectorizer, number_words)  \n",
        "\n",
        "  return lda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHm63GMi7jEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e9f24b87-303e-46b9-dfd6-f0403cfc86d1"
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 1))\n",
        "count_data = count_vectorizer.fit_transform(df['text_processed'])\n",
        "\n",
        "#tfidf_vectorizer = TfidfVectorizer(**count_vectorizer.get_params())\n",
        "#dtm_tfidf = tfidf_vectorizer.fit_transform(df['text_processed'])\n",
        "\n",
        "Topic_Coherence = TopicCoherence()\n",
        "Topic_Coherence.fit(count_vectorizer.vocabulary, count_data, True)\n",
        "\n",
        "data_dense = count_data.todense()\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting...\n",
            "Di done\n",
            "Dij done\n",
            "pi and pij done\n",
            "Sparsicity:  13.039283991639422 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dfIErtQ8n5E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "bc52e164-6633-4465-a613-10c9b2915057"
      },
      "source": [
        "topics = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "\n",
        "metricsDF = pd.DataFrame(topics, columns=['nTopic'])\n",
        "likelihoods = []\n",
        "perplexities = []\n",
        "coherences = []\n",
        "params = []\n",
        "\n",
        "#for topic in topics:\n",
        "#  lda_tfidf = generate_topics(tfidf_vectorizer, dtm_tfidf, topic, 10)\n",
        "#  joblib.dump(lda_tfidf, \"/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/models/td-idf-topics-{}.jl\".format(topic))\n",
        "#  preparedData = pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)\n",
        "#  pyLDAvis.save_html(preparedData, \"/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/html/td-idf-topics-{}.html\".format(topic))\n",
        "#  classification = lda_tfidf.transform(dtm_tfidf)\n",
        "#  topicsvalues = []\n",
        "#  for topicnum in classification:\n",
        "#    max_value = max(topicnum)\n",
        "#    i, = np.where(np.isclose(topicnum, max_value))\n",
        "#    topicsvalues.append(i[0] + 1)\n",
        "\n",
        "#  df[\"topic-for-td-idf-{}-model\".format(topic)] = topicsvalues\n",
        "\n",
        "for topic in topics:\n",
        "  lda = generate_topics(count_vectorizer, count_data, topic, 10)\n",
        "  joblib.dump(lda, \"/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/models/frequency-topics-{}.jl\".format(topic))\n",
        "  preparedData = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer)\n",
        "  pyLDAvis.save_html(preparedData, \"/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/html/frequency-topics-{}.html\".format(topic))\n",
        "  classification = lda.transform(count_data)\n",
        "  topicsvalues = []\n",
        "  for topicnum in classification:\n",
        "    max_value = max(topicnum)\n",
        "    i, = np.where(np.isclose(topicnum, max_value))\n",
        "    topicsvalues.append(i[0] + 1)\n",
        "\n",
        "  df[\"topic-for-frequency-{}-model\".format(topic)] = topicsvalues\n",
        "  likelihoods.append(lda.score(count_data))\n",
        "  perplexities.append(lda.perplexity(count_data))\n",
        "  coherences.append(Topic_Coherence.myTCScore(lda))\n",
        "  params.append(lda.get_params())\n",
        "\n",
        "metricsDF['likelihood'] = likelihoods\n",
        "metricsDF['perplexity'] = perplexities\n",
        "metricsDF['tcoherence'] = coherences\n",
        "metricsDF['params'] = params\n",
        "\n",
        "df.to_csv(r'/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/lda-result.csv', index = False)\n",
        "metricsDF.to_csv(r'/content/drive/My Drive/PedroAlmir/09_Doutorado/UFC/projetos/16_PhdProposal/review/topicModeling/lda-metrics.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.30\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.31\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.29\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.31\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.28\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.28\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.31\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.29\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.28\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.30\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.30\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.26\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean both Score: 0.26\r"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}